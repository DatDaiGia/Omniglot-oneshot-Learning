{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport numpy as np\nfrom sklearn.utils import shuffle\nimport cv2\nimport numpy.random as random\nfrom keras.models import Model, Input, Sequential\nfrom keras.layers.core import Dense, Flatten, Lambda\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.regularizers import l2\nimport keras.backend as K\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Constant variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DIR = \"../input/omniglot-dataset/images_background/images_background/\"\nTEST_DIR = \"../input/omniglot-dataset/images_evaluation/images_evaluation/\"\nBATCH_SIZE = 32\nHEIGHT = 105\nWIDTH = 105\nCHANNEL = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data generation"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGeneration:\n    def __init__(self):\n        self.train_dir = TRAIN_DIR\n        self.test_dir = TEST_DIR\n        self.batch_size = BATCH_SIZE\n        \n    def load_image(self, img_path):\n        image = cv2.imread(img_path, 0)\n        return image\n    \n    def load_dataset(self, data_path, current_y=0):\n        X = []\n        y = []\n        lang_dict = {}\n        current_y = current_y\n        \n        for alphabet in os.listdir(data_path):\n            print(\"Loading {}\".format(alphabet))\n            alphabet_path = os.path.join(data_path, alphabet)\n            lang_dict[alphabet] = [current_y, None]\n            \n            for letter in os.listdir(alphabet_path):\n                letter_path = os.path.join(alphabet_path, letter)\n                categories_images = []\n                \n                for img in os.listdir(letter_path):\n                    img_path = os.path.join(letter_path, img)\n                    image = self.load_image(img_path)\n                    categories_images.append(image)\n                    y.append(current_y)\n                \n                try:\n                    X.append(np.stack(categories_images))\n                except ValueError as e:\n                    print(e)\n                    print(\"error - category_images:\", category_images)\n                \n                lang_dict[alphabet][1]=current_y\n                current_y += 1\n               \n        X = np.stack(X)\n        y = np.vstack(y)\n        \n        return X, y, lang_dict\n\n    def get_batch(self, X, y):\n        n_classes, n_samples, w, h = X.shape\n        categories = random.choice(n_classes, size=(self.batch_size,), replace=False)\n        targets = np.zeros((self.batch_size,))\n        pairs = [np.zeros((self.batch_size, h, w, 1)) for _ in range(2)]\n        targets[self.batch_size//2:] = 1\n        \n        for i in range(self.batch_size):\n            category = categories[i]\n            idx_1 = random.randint(0, n_samples)\n            pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n            idx_2 = random.randint(0, n_samples)\n            \n            if i > self.batch_size//2:\n                category_2 = category\n            else:\n                category_2 = (category + random.randint(1, n_classes))%n_classes\n                \n            pairs[1][i,:,:,:] = X[category_2, idx_2].reshape(w, h, 1)\n        \n        return pairs, targets\n\n    def generate(self, X, y):\n        while True:\n            pairs, targets = self.get_batch(X, y)\n            yield (pairs, targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_gener = DataGeneration()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train, train_categories = data_gener.load_dataset(TRAIN_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test, y_test, test_categories = data_gener.load_dataset(TEST_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_weights(shape, dtype=None):\n    return np.random.normal(loc=0.0, scale=1e-2, size=shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_bias(shape, dtype=None):\n    return np.random.normal(loc=0.5, scale=1e-2, size=shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_siamese_net(input_shape):\n    # Build input\n    left_input = Input(input_shape)\n    right_input = Input(input_shape)\n\n    # Model architecture of features extraction\n    model = Sequential()\n    model.add(Conv2D(64, (10, 10), activation=\"relu\",\n                     input_shape=input_shape,\n                     kernel_initializer=initialize_weights,\n                     kernel_regularizer=l2(1e-4)))\n    model.add(MaxPooling2D())\n    model.add(Conv2D(128, (7, 7), activation=\"relu\",\n                     kernel_initializer=initialize_weights,\n                     bias_initializer=initialize_bias,\n                     kernel_regularizer=l2(1e-4)))\n    model.add(MaxPooling2D())\n    model.add(Conv2D(128, (4, 4), activation=\"relu\",\n                     kernel_initializer=initialize_weights,\n                     bias_initializer=initialize_bias,\n                     kernel_regularizer=l2(1e-4)))\n    model.add(MaxPooling2D())\n    model.add(Conv2D(256, (4, 4), activation=\"relu\",\n                     kernel_initializer=initialize_weights,\n                     bias_initializer=initialize_bias,\n                     kernel_regularizer=l2(1e-4)))\n    model.add(Flatten())\n    model.add(Dense(4096, activation=\"sigmoid\",\n                    kernel_initializer=initialize_weights,\n                    bias_initializer=initialize_bias,\n                    kernel_regularizer=l2(1e-3)))\n    \n    # Encoding inputs\n    left_encoding = model(left_input)\n    right_encoding = model(right_input)\n    \n    # Add a layer to compute distance\n    L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n    L1_distance = L1_layer([left_encoding, right_encoding])\n    \n    # Add dense layer to predict\n    prediction = Dense(1, activation=\"sigmoid\", bias_initializer=initialize_bias)(L1_distance)\n    \n    #Connect inputs and output\n    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n    \n    return siamese_net","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_siamese_net((WIDTH, HEIGHT, CHANNEL))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = Adam(lr = 0.00006)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_n_test(N, s=\"val\", language=None):\n    if s==\"train\":\n        X = X_train\n        categories = train_categories\n    else:\n        X = X_test\n        categories = test_categories\n        \n    n_classes, n_samples, w, h = X.shape\n        \n    if language is not None:\n        low, high = categories[language]\n        if high - low < N:\n            raise ValueError(\"This language {} has less than {} letters\".format(language, N))\n        categories = random.choice(range(low, high), size=N, replace=False)\n    else:\n        categories = random.choice(range(n_classes), size=N, replace=False)\n    \n    true_category = categories[0]\n    ex_1, ex_2 = random.choice(range(n_samples), size=2, replace=False)\n    main_set = np.asarray([X[true_category, ex_1, :, :]]*N).reshape((N, w, h, 1))\n    support_set = np.asarray(X[categories, ex_1, :, :])\n    support_set[0] = X[true_category, ex_2, :, :]\n    support_set = support_set.reshape(N, w, h, 1)\n    targets = np.zeros((N,))\n    targets[0] = 1\n    targets, main_set, support_set = shuffle(targets, main_set, support_set)\n    pairs = [main_set, support_set]\n    \n    return pairs, targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_one_shot(model, N, k, s=\"val\", verbose=0):\n    n_correct = 0\n    \n    if verbose == 0:\n        print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n    \n    for i in range(k):\n        inputs, targets = make_n_test(N, s)\n        probs = model.predict(inputs)\n        \n        if probs.argmax() == targets.argmax():\n              n_correct += 1\n    percent_correct = (100.0 * n_correct / k)\n\n    if verbose:\n        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n    return percent_correct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_every = 200 # interval for evaluating on one-shot tasks\nn_iter = 60000 # No. of training iterations\nN_way = 20 # how many classes for testing one-shot tasks\nn_val = 250 # how many one-shot tasks to validate on\nbest = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf \"./weights/\"\nmodel_path = './weights/'\n\nif not os.path.exists(model_path):\n    os.mkdir(model_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nprint(\"Starting training process!\")\nprint(\"-------------------------------------\")\n\nt_start = time.time()\nfor i in range(1, n_iter+1):\n#     print(data_generation.get_batch(X_train, y_train))\n    inputs, targets = data_gener.get_batch(X_train, y_train)\n    loss = model.train_on_batch(inputs, targets)\n    \n    if i % evaluate_every == 0:\n        print(\"\\n ------------- \\n\")\n        print(\"Time for {0} iterations: {1} mins\".format(i, (time.time() - t_start)/60.0))\n        print(\"Train Loss: {0}\".format(loss)) \n        val_acc = test_one_shot(model, N_way, n_val, verbose=True)\n        if val_acc > 86:\n            model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n        \n        if val_acc >= best:\n            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n            best = val_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}